# -*- coding: utf-8 -*-
"""PIEZOELECTRIC.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1Cipg8YtTcU5z2tQIttaf0XKx9oj10atv
"""

import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestRegressor
from sklearn.metrics import mean_squared_error, r2_score

# Load KNN Dataset
knn_data = pd.read_csv("knn.csv")  # Update with actual file name

# Display basic info
print(knn_data.head())
print(knn_data.info())

# Correlation Matrix
plt.figure(figsize=(10, 6))
sns.heatmap(knn_data.corr(), annot=True, cmap='coolwarm')
plt.title("Correlation Matrix")
plt.show()

# Feature Selection (Modify as needed)
X = knn_data[['avg_speed(m/s)', 'max_speed(m/s)', 'd33(C)', 'Acceleration(m/s)', 'Force', 'C(F)']]
y = knn_data['P(Watt)']  # Target variable

# Split Data
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Train Model
model = RandomForestRegressor(n_estimators=100, random_state=42)
model.fit(X_train, y_train)

# Predictions
y_pred = model.predict(X_test)

# Evaluate Model
mse = mean_squared_error(y_test, y_pred)
r2 = r2_score(y_test, y_pred)
print(f"Mean Squared Error: {mse}")
print(f"R2 Score: {r2}")

# Feature Importance
plt.figure(figsize=(8, 5))
importances = model.feature_importances_
sns.barplot(x=X.columns, y=importances)
plt.title("Feature Importance for Power Prediction")
plt.xticks(rotation=45)
plt.show()

# Power vs Speed Plot
plt.figure(figsize=(8, 5))
sns.scatterplot(x=knn_data['avg_speed(m/s)'], y=knn_data['P(Watt)'])
plt.xlabel("Average Speed (m/s)")
plt.ylabel("Power (Watt)")
plt.title("Power vs Average Speed")
plt.show()

# Splitting data for modeling
features = ['speed(m/s)', 'd33(pC/N)', 'Acceleration(m/s)', 'Force', 'C(F)']
target = 'P(Watt)'

# Assuming 'knn_data' is your DataFrame
df = knn_data.copy()  # Create a copy to avoid modifying the original DataFrame
# If 'speed(m/s)' is not in knn_data and you meant 'avg_speed(m/s)', change it in 'features' list.
# If 'd33(pC/N)' is not in knn_data and you meant 'd33(C)', change it in 'features' list.
# ... (similarly for other features if needed)


# Check if the required columns are present in the DataFrame
for feature in features:
    if feature not in df.columns:
        print(f"Warning: Feature '{feature}' not found in DataFrame columns.")
        # Handle the missing feature, e.g., raise an error or replace with a similar feature

if target not in df.columns:
    raise ValueError(f"Target variable '{target}' not found in DataFrame columns.")

X_train, X_test, y_train, y_test = train_test_split(df[features], df[target], test_size=0.2, random_state=42)

# Train a model
model = RandomForestRegressor(n_estimators=100, random_state=42)
model.fit(X_train, y_train)

# Predict and evaluate
y_pred = model.predict(X_test)

# Import mean_absolute_error if not already imported
from sklearn.metrics import mean_absolute_error
mae = mean_absolute_error(y_test, y_pred)
r2 = r2_score(y_test, y_pred)
print(f"Mean Absolute Error: {mae:.4f}, R2 Score: {r2:.4f}")

# SHAP Analysis
# Install shap if not already installed
!pip install shap
import shap

explainer = shap.Explainer(model, X_train)
shap_values = explainer(X_test)
shap.summary_plot(shap_values, X_test)

import pandas as pd
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestRegressor
from sklearn.metrics import mean_absolute_error, r2_score
import shap
# Load the PZT dataset (replace with actual file path)
pzt_data = pd.read_csv("pzt.csv")

# Display dataset info
print(pzt_data.info())

# Check for missing values
print(pzt_data.isnull().sum())

# Convert columns with 'ER=600' to numeric, handling errors
for column in pzt_data.columns:
    try:
        pzt_data[column] = pd.to_numeric(pzt_data[column], errors='coerce')  # Convert to numeric, replace non-numeric with NaN
    except (ValueError, TypeError):
        print(f"Warning: Could not convert column '{column}' to numeric.")
# Correlation heatmap
plt.figure(figsize=(12, 6))
sns.heatmap(pzt_data.corr(), annot=True, cmap="coolwarm", linewidths=0.5)
plt.title("Correlation Heatmap - PZT Data")
plt.show()

# Define features and target
X = pzt_data.drop(columns=['P(Watt)'])  # Features
y = pzt_data['P(Watt)']  # Target variable

# Split dataset into training and testing
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Train Random Forest model
model = RandomForestRegressor(n_estimators=100, random_state=42)
model.fit(X_train, y_train)

# Predictions
y_pred = model.predict(X_test)

# Model evaluation
mae = mean_absolute_error(y_test, y_pred)
r2 = r2_score(y_test, y_pred)
print(f"Mean Absolute Error: {mae}")
print(f"RÂ² Score: {r2}")

# SHAP analysis
explainer = shap.Explainer(model, X_train)
shap_values = explainer(X_test)

# SHAP summary plot
shap.summary_plot(shap_values, X_test)

# Plot Power (Watt) vs Distance
plt.figure(figsize=(10, 5))
sns.scatterplot(x=pzt_data['distance'], y=pzt_data['P(Watt)'], color='blue')
plt.xlabel("Distance (m)")
plt.ylabel("Power (Watt)")
plt.title("Power vs Distance - PZT Material")
plt.show()

# Feature Importance
feature_importances = pd.Series(model.feature_importances_, index=X.columns)
feature_importances.sort_values(ascending=False).plot(kind='bar', figsize=(10,5), color='orange')
plt.title("Feature Importance - PZT Material")
plt.show()

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestRegressor
import shap

# Load PMN dataset (Replace 'pmn_data.csv' with your actual file)
pdf = pd.read_csv('pmn.csv')

# Display basic information
print(pdf.info())
print(pdf.describe())

# Check for missing values

print("Missing Values:\n", pdf.isnull().sum())

# Remove Unnamed columns before converting to numeric
pdf = pdf.loc[:, ~pdf.columns.str.startswith('Unnamed')]

for column in pdf.columns:
    pdf[column] = pd.to_numeric(pdf[column], errors='coerce')  # Convert to numeric, replacing errors with NaN
    # pdf = pdf.select_dtypes(include=np.number)  # No need to select dtypes again

# Visualize correlations
plt.figure(figsize=(12, 6))
sns.heatmap(pdf.corr(), annot=True, cmap='coolwarm', linewidths=0.5)
plt.title("Feature Correlation Heatmap")
plt.show()

# Define features and target variable
X = pdf.drop(columns=['P(Watt)'])  # Replace 'Power' with actual output column name
y = pdf['P(Watt)']

# Split into training and testing data
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Train a RandomForestRegressor model
model = RandomForestRegressor(n_estimators=100, random_state=42)
model.fit(X_train, y_train)

# Feature Importance Visualization
feature_importance = pd.Series(model.feature_importances_, index=X.columns)
feature_importance.sort_values(ascending=False).plot(kind='bar', figsize=(10, 5))
plt.title("Feature Importance in PMN-PT Analysis")
plt.show()

# SHAP Analysis for explainability
explainer = shap.Explainer(model, X_train)
shap_values = explainer(X_test)
shap.summary_plot(shap_values, X_test)

# Plot Power (Watt) vs Distance
plt.figure(figsize=(10, 5))
sns.scatterplot(x=pzt_data['distance'], y=pzt_data['P(Watt)'], color='blue')
plt.xlabel("Distance (m)")
plt.ylabel("Power (Watt)")
plt.title("Power vs Distance - PZT Material")
plt.show()

# Feature Importance
feature_importances = pd.Series(model.feature_importances_, index=X.columns)
# Plot Power (Watt) vs Distance
plt.figure(figsize=(10, 5))
sns.scatterplot(x=pzt_data['distance'], y=pzt_data['P(Watt)'], color='blue')
plt.xlabel("Distance (m)")
plt.ylabel("Power (Watt)")
plt.title("Power vs Distance - PZT Material")
plt.show()

# Feature Importance
feature_importances = pd.Series(model.feature_importances_, index=X.columns)
feature_importances.sort_values(ascending=False).plot(kind='bar', figsize=(10,5), color='orange')
plt.title("Feature Importance - PZT Material")
plt.show()
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestRegressor
import shap

# Load PMN dataset (Replace 'pmn_data.csv' with your actual file)
pdf = pd.read_csv('pmn.csv')

# Display basic information
print(pdf.info())
print(pdf.describe())

# Check for missing values

print("Missing Values:\n", pdf.isnull().sum())

# Remove Unnamed columns before converting to numeric
pdf = pdf.loc[:, ~pdf.columns.str.startswith('Unnamed')]

for column in pdf.columns:
    pdf[column] = pd.to_numeric(pdf[column], errors='coerce')  # Convert to numeric, replacing errors with NaN
    # pdf = pdf.select_dtypes(include=np.number)  # No need to select dtypes again

# Visualize correlations
plt.figure(figsize=(12, 6))
sns.heatmap(pdf.corr(), annot=True, cmap='coolwarm', linewidths=0.5)
plt.title("Feature Correlation Heatmap")
plt.show()

# Define features and target variable
X = pdf.drop(columns=['P(Watt)'])  # Replace 'Power' with actual output column name
y = pdf['P(Watt)']

# Split into training and testing data
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Train a RandomForestRegressor model
model = RandomForestRegressor(n_estimators=100, random_state=42)
model.fit(X_train, y_train)

# Feature Importance Visualization
feature_importance = pd.Series(model.feature_importances_, index=X.columns)
feature_importance.sort_values(ascending=False).plot(kind='bar', figsize=(10, 5))
plt.title("Feature Importance in PMN Analysis")
plt.show()

# SHAP Analysis for explainability
explainer = shap.Explainer(model, X_train)
shap_values = explainer(X_test)
shap.summary_plot(shap_values, X_test)
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

# Load datasets for KNN, PZT, and PMN
knn_data = pd.read_csv('knn.csv')  # Replace with actual file path
pzt_data = pd.read_csv('pzt.csv')  # Replace with actual file path
pmn_data = pd.read_csv('pmn.csv')  # Replace with actual file path

# Extract relevant columns
data = {
    "Material": ["KNN"] * len(knn_data) + ["PZT"] * len(pzt_data) + ["PMN-PT"] * len(pmn_data),
    "Power (1 sec)": knn_data["P(Watt)"].tolist() + pzt_data["P(Watt)"].tolist() + pmn_data["P(Watt)"].tolist(),
    "Power (1 day)": knn_data["P (per day)"].tolist() + pzt_data["P (per day)"].tolist() + pmn_data["P (per day)"].tolist(),
    "d33 (pC/N)": knn_data["d33(pC/N)"].tolist() + pzt_data["d33(pC/N)"].tolist() + pmn_data["d33(pC/N)"].tolist()
}
comparison_df = pd.DataFrame(data)

# Plot Power Comparison
plt.figure(figsize=(12, 6))
sns.boxplot(x="Material", y="Power (1 sec)", data=comparison_df)
plt.title("Power Generated in 1 Second for Different Materials")
plt.show()

plt.figure(figsize=(12, 6))
sns.boxplot(x="Material", y="Power (1 day)", data=comparison_df)
plt.title("Power Generated in 1 Day for Different Materials")
plt.show()

# Plot d33 comparison
plt.figure(figsize=(12, 6))
sns.boxplot(x="Material", y="d33 (pC/N)", data=comparison_df)
plt.title("d33 Value Comparison for Different Materials")
plt.show()

# Summary statistics
summary_stats = comparison_df.groupby("Material").agg({
    "Power (1 sec)": ["mean", "max", "min"],
    "Power (1 day)": ["mean", "max", "min"],
    "d33 (pC/N)": ["mean", "max", "min"]
})
print(summary_stats)
import seaborn as sns
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestRegressor
import shap

# Load PMN dataset (Replace 'pmn_data.csv' with your actual file)
pdf = pd.read_csv('pmn.csv')

# Display basic information
print(pdf.info())
print(pdf.describe())

# Check for missing values

print("Missing Values:\n", pdf.isnull().sum())

# Remove Unnamed columns before converting to numeric
pdf = pdf.loc[:, ~pdf.columns.str.startswith('Unnamed')]

for column in pdf.columns:
    pdf[column] = pd.to_numeric(pdf[column], errors='coerce')  # Convert to numeric, replacing errors with NaN
    # pdf = pdf.select_dtypes(include=np.number)  # No need to select dtypes again

# Visualize correlations
plt.figure(figsize=(12, 6))
sns.heatmap(pdf.corr(), annot=True, cmap='coolwarm', linewidths=0.5)
plt.title("Feature Correlation Heatmap")
plt.show()

# Define features and target variable
X = pdf.drop(columns=['P(Watt)'])  # Replace 'Power' with actual output column name
y = pdf['P(Watt)']

# Split into training and testing data
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Train a RandomForestRegressor model
model = RandomForestRegressor(n_estimators=100, random_state=42)
model.fit(X_train, y_train)

# Feature Importance Visualization
feature_importance = pd.Series(model.feature_importances_, index=X.columns)
feature_importance.sort_values(ascending=False).plot(kind='bar', figsize=(10, 5))
plt.title("Feature Importance in PMN Analysis")
plt.show()

# SHAP Analysis for explainability
explainer = shap.Explainer(model, X_train)
shap_values = explainer(X_test)
shap.summary_plot(shap_values, X_test)

import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

# Load datasets for KNN, PZT, and PMN
knn_data = pd.read_csv('knn.csv')  # Replace with actual file path
pzt_data = pd.read_csv('pzt.csv')  # Replace with actual file path
pmn_data = pd.read_csv('pmn.csv')  # Replace with actual file path

# Extract relevant columns
data = {
    "Material": ["KNN"] * len(knn_data) + ["PZT"] * len(pzt_data) + ["PMN-PT"] * len(pmn_data),
    "Power (1 sec)": knn_data["P(Watt)"].tolist() + pzt_data["P(Watt)"].tolist() + pmn_data["P(Watt)"].tolist(),
    "Power (1 day)": knn_data["P (per day)"].tolist() + pzt_data["P (per day)"].tolist() + pmn_data["P (per day)"].tolist(),
    "d33 (pC/N)": knn_data["d33(pC/N)"].tolist() + pzt_data["d33(pC/N)"].tolist() + pmn_data["d33(pC/N)"].tolist()
}
comparison_df = pd.DataFrame(data)

# Plot Power Comparison
plt.figure(figsize=(12, 6))
sns.boxplot(x="Material", y="Power (1 sec)", data=comparison_df)
plt.title("Power Generated in 1 Second for Different Materials")
plt.show()

plt.figure(figsize=(12, 6))
sns.boxplot(x="Material", y="Power (1 day)", data=comparison_df)
plt.title("Power Generated in 1 Day for Different Materials")
plt.show()

# Plot d33 comparison
plt.figure(figsize=(12, 6))
sns.boxplot(x="Material", y="d33 (pC/N)", data=comparison_df)
plt.title("d33 Value Comparison for Different Materials")
plt.show()

# Summary statistics
summary_stats = comparison_df.groupby("Material").agg({
    "Power (1 sec)": ["mean", "max", "min"],
    "Power (1 day)": ["mean", "max", "min"],
    "d33 (pC/N)": ["mean", "max", "min"]
})
print(summary_stats)